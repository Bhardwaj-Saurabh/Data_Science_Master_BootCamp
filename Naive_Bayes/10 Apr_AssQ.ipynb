{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMkZEAxYIu9Epvr4k6nB5pl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?**\n","\n","**Answer:**\n","\n","Let S be the event that an employee is a smoker, and H be the event that the employee uses the health insurance plan. \n","\n","Calculate P(S|H), the probability that an employee is a smoker given that he/she uses the health insurance plan.\n","\n","Bayes' theorem:\n","\n","P(S|H) = P(H|S) * P(S) / P(H)\n","\n","Calculate: \n","\n","P(H) = 0.7\n","\n","P(S and H) = P(S|H) * P(H) = P(H|S) * P(S)\n","\n","P(H|S) = P(S and H) / P(S) = (0.4 * 0.7) / P(S)\n","\n","so,\n","\n","P(S|H) = P(H|S) * P(S) / P(H) = (0.4 * 0.7) / 0.7 = 0.4\n","\n","The probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4 or 40%.\n","\n"],"metadata":{"id":"9wlD8u7l64Ik"}},{"cell_type":"markdown","source":["**Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?**\n","\n","**Answer:**\n","\n","The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in how they represent the input features. Bernoulli Naive Bayes assumes that the input features are binary (i.e., 0 or 1) while Multinomial Naive Bayes assumes that the input features are counts of occurrences (i.e., non-negative integers).\n","\n","In Bernoulli Naive Bayes, each feature is treated as a binary variable indicating whether or not it occurs in the document. For example, in a text classification problem, each feature could represent the presence or absence of a particular word in the document. The probability of each feature given the class is modeled using a Bernoulli distribution.\n","\n","In Multinomial Naive Bayes, each feature represents the count of occurrences of a particular word in the document. The probability of each feature given the class is modeled using a Multinomial distribution.\n","\n","In summary, Bernoulli Naive Bayes assumes binary features and uses a Bernoulli distribution to model the probabilities of the features given the class, while Multinomial Naive Bayes assumes count features and uses a Multinomial distribution to model the probabilities of the features given the class."],"metadata":{"id":"4OL3Q2sb64D0"}},{"cell_type":"markdown","source":["**Q3. How does Bernoulli Naive Bayes handle missing values?**\n","\n","**Answer:**\n","\n","In Bernoulli Naive Bayes, missing values can be handled by simply ignoring them and treating them as if they were never present. This is because the algorithm only considers whether or not a feature is present (i.e., binary) and does not take into account the magnitude or value of the feature.\n","\n","When training the model, any instances with missing values can be excluded from the training set or imputed with some default value (e.g., 0 or 1). The choice of imputation method can have an impact on the model's performance, and should be chosen based on the specific problem and dataset."],"metadata":{"id":"g3iidpcD63_7"}},{"cell_type":"markdown","source":["**Q4. Can Gaussian Naive Bayes be used for multi-class classification?**\n","\n","**Answer:**\n","\n","Yes, Gaussian Naive Bayes can be used for multi-class classification. In multi-class classification, the goal is to predict a target variable with three or more possible outcomes, and the Gaussian Naive Bayes algorithm can be used to make these predictions.\n","\n","To use Gaussian Naive Bayes for multi-class classification, the algorithm extends the binary classification approach by building a separate model for each class. For example, if there are three classes (A, B, and C), then the algorithm would build three separate models, one for each class.\n","\n","\n"],"metadata":{"id":"6O5P1yg9637U"}},{"cell_type":"markdown","source":["**Q5. Assignment:**\n","\n","**Data preparation:**\n","\n","Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). \n","\n","This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n","\n","**Implementation:**\n","\n","Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using thescikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n","dataset. You should use the default hyperparameters for each classifier.\n","\n","**Results:**\n","\n","Report the following performance metrics for each classifier:\n","\n","Accuracy\n","\n","Precision\n","\n","Recall\n","\n","F1 score\n","\n","**Discussion:**\n","\n","Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n","\n","**Conclusion:**\n","\n","Summarise your findings and provide some suggestions for future work."],"metadata":{"id":"U1hJmZCA633U"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6jTeingf6yIw","executionInfo":{"status":"ok","timestamp":1681462222144,"user_tz":-60,"elapsed":1001,"user":{"displayName":"Saurabh Bhardwaj","userId":"16519378442147256210"}},"outputId":"d8ac0183-cc25-45ad-fce6-7fb47f22b930"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('spambase.csv', <http.client.HTTPMessage at 0x7f4c24fa0b80>)"]},"metadata":{},"execution_count":1}],"source":["import urllib.request\n","\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n","filename = \"spambase.csv\"\n","\n","urllib.request.urlretrieve(url, filename)"]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the dataset\n","names = pd.read_csv('spambase.names', skiprows=32, sep=':\\\\s+', engine='python', names=['attr', ''])\n","names = names['attr']\n","names = list(names)\n","names.append('label')\n","df = pd.read_csv('spambase.csv', names=names)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"fs68FWek9Mx_","executionInfo":{"status":"ok","timestamp":1681462688663,"user_tz":-60,"elapsed":266,"user":{"displayName":"Saurabh Bhardwaj","userId":"16519378442147256210"}},"outputId":"092c02a4-19a1-40d1-ac6e-fbe36fa75e96"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n","0            0.00               0.64           0.64           0.0   \n","1            0.21               0.28           0.50           0.0   \n","2            0.06               0.00           0.71           0.0   \n","3            0.00               0.00           0.00           0.0   \n","4            0.00               0.00           0.00           0.0   \n","\n","   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n","0           0.32            0.00              0.00                0.00   \n","1           0.14            0.28              0.21                0.07   \n","2           1.23            0.19              0.19                0.12   \n","3           0.63            0.00              0.31                0.63   \n","4           0.63            0.00              0.31                0.63   \n","\n","   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n","0             0.00            0.00  ...         0.00        0.000   \n","1             0.00            0.94  ...         0.00        0.132   \n","2             0.64            0.25  ...         0.01        0.143   \n","3             0.31            0.63  ...         0.00        0.137   \n","4             0.31            0.63  ...         0.00        0.135   \n","\n","   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n","0          0.0        0.778        0.000        0.000   \n","1          0.0        0.372        0.180        0.048   \n","2          0.0        0.276        0.184        0.010   \n","3          0.0        0.137        0.000        0.000   \n","4          0.0        0.135        0.000        0.000   \n","\n","   capital_run_length_average  capital_run_length_longest  \\\n","0                       3.756                          61   \n","1                       5.114                         101   \n","2                       9.821                         485   \n","3                       3.537                          40   \n","4                       3.537                          40   \n","\n","   capital_run_length_total  label  \n","0                       278      1  \n","1                      1028      1  \n","2                      2259      1  \n","3                       191      1  \n","4                       191      1  \n","\n","[5 rows x 58 columns]"],"text/html":["\n","  <div id=\"df-edd2aeed-adda-49f2-b927-c206dc86c94c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word_freq_make</th>\n","      <th>word_freq_address</th>\n","      <th>word_freq_all</th>\n","      <th>word_freq_3d</th>\n","      <th>word_freq_our</th>\n","      <th>word_freq_over</th>\n","      <th>word_freq_remove</th>\n","      <th>word_freq_internet</th>\n","      <th>word_freq_order</th>\n","      <th>word_freq_mail</th>\n","      <th>...</th>\n","      <th>char_freq_;</th>\n","      <th>char_freq_(</th>\n","      <th>char_freq_[</th>\n","      <th>char_freq_!</th>\n","      <th>char_freq_$</th>\n","      <th>char_freq_#</th>\n","      <th>capital_run_length_average</th>\n","      <th>capital_run_length_longest</th>\n","      <th>capital_run_length_total</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.64</td>\n","      <td>0.64</td>\n","      <td>0.0</td>\n","      <td>0.32</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.000</td>\n","      <td>0.0</td>\n","      <td>0.778</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>3.756</td>\n","      <td>61</td>\n","      <td>278</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.21</td>\n","      <td>0.28</td>\n","      <td>0.50</td>\n","      <td>0.0</td>\n","      <td>0.14</td>\n","      <td>0.28</td>\n","      <td>0.21</td>\n","      <td>0.07</td>\n","      <td>0.00</td>\n","      <td>0.94</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.132</td>\n","      <td>0.0</td>\n","      <td>0.372</td>\n","      <td>0.180</td>\n","      <td>0.048</td>\n","      <td>5.114</td>\n","      <td>101</td>\n","      <td>1028</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.06</td>\n","      <td>0.00</td>\n","      <td>0.71</td>\n","      <td>0.0</td>\n","      <td>1.23</td>\n","      <td>0.19</td>\n","      <td>0.19</td>\n","      <td>0.12</td>\n","      <td>0.64</td>\n","      <td>0.25</td>\n","      <td>...</td>\n","      <td>0.01</td>\n","      <td>0.143</td>\n","      <td>0.0</td>\n","      <td>0.276</td>\n","      <td>0.184</td>\n","      <td>0.010</td>\n","      <td>9.821</td>\n","      <td>485</td>\n","      <td>2259</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.63</td>\n","      <td>0.00</td>\n","      <td>0.31</td>\n","      <td>0.63</td>\n","      <td>0.31</td>\n","      <td>0.63</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.137</td>\n","      <td>0.0</td>\n","      <td>0.137</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>3.537</td>\n","      <td>40</td>\n","      <td>191</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.63</td>\n","      <td>0.00</td>\n","      <td>0.31</td>\n","      <td>0.63</td>\n","      <td>0.31</td>\n","      <td>0.63</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.135</td>\n","      <td>0.0</td>\n","      <td>0.135</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>3.537</td>\n","      <td>40</td>\n","      <td>191</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 58 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-edd2aeed-adda-49f2-b927-c206dc86c94c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-edd2aeed-adda-49f2-b927-c206dc86c94c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-edd2aeed-adda-49f2-b927-c206dc86c94c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Create feature and labels\n","X = df.drop('label', axis=1)\n","y = df['label']"],"metadata":{"id":"O7rfslSq-8wI","executionInfo":{"status":"ok","timestamp":1681463609683,"user_tz":-60,"elapsed":217,"user":{"displayName":"Saurabh Bhardwaj","userId":"16519378442147256210"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Create Bernoulli Naive Bayes classifier and evaluate its performance using 10-fold cross-validation\n","bnb = BernoulliNB()\n","bnb.fit(X, y)\n","bnb_scores = cross_val_score(bnb, X, y, cv=10)\n","bnb_accuracy = bnb_scores.mean()\n","\n","# Create Multinomial Naive Bayes classifier and evaluate its performance using 10-fold cross-validation\n","mnb = MultinomialNB()\n","mnb.fit(X, y)\n","mnb_scores = cross_val_score(mnb, X, y, cv=10)\n","mnb_accuracy = mnb_scores.mean()\n","\n","# Create Gaussian Naive Bayes classifier and evaluate its performance using 10-fold cross-validation\n","gnb = GaussianNB()\n","gnb.fit(X, y)\n","gnb_scores = cross_val_score(gnb, X, y, cv=10)\n","gnb_accuracy = gnb_scores.mean()"],"metadata":{"id":"XgDOwQQT9sH-","executionInfo":{"status":"ok","timestamp":1681463611849,"user_tz":-60,"elapsed":964,"user":{"displayName":"Saurabh Bhardwaj","userId":"16519378442147256210"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Calculate precision, recall, and F1 score for each classifier using the default decision threshold\n","bnb_precision = precision_score(y, bnb.predict(X))\n","bnb_recall = recall_score(y, bnb.predict(X))\n","bnb_f1 = f1_score(y, bnb.predict(X))\n","\n","mnb_precision = precision_score(y, mnb.predict(X))\n","mnb_recall = recall_score(y, mnb.predict(X))\n","mnb_f1 = f1_score(y, mnb.predict(X))\n","\n","gnb_precision = precision_score(y, gnb.predict(X))\n","gnb_recall = recall_score(y, gnb.predict(X))\n","gnb_f1 = f1_score(y, gnb.predict(X))"],"metadata":{"id":"gXwO694c_N-i","executionInfo":{"status":"ok","timestamp":1681463612361,"user_tz":-60,"elapsed":236,"user":{"displayName":"Saurabh Bhardwaj","userId":"16519378442147256210"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Print the performance metrics for each classifier\n","print(\"Bernoulli Naive Bayes Accuracy:\", bnb_accuracy)\n","print(\"Bernoulli Naive Bayes Precision:\", bnb_precision)\n","print(\"Bernoulli Naive Bayes Recall:\", bnb_recall)\n","print(\"Bernoulli Naive Bayes F1 Score:\", bnb_f1)\n","print('---------------------------------------------------------')\n","print(\"Multinomial Naive Bayes Accuracy:\", mnb_accuracy)\n","print(\"Multinomial Naive Bayes Precision:\", mnb_precision)\n","print(\"Multinomial Naive Bayes Recall:\", mnb_recall)\n","print(\"Multinomial Naive Bayes F1 Score:\", mnb_f1)\n","print('---------------------------------------------------------')\n","print(\"Gaussian Naive Bayes Accuracy:\", gnb_accuracy)\n","print(\"Gaussian Naive Bayes Precision:\", gnb_precision)\n","print(\"Gaussian Naive Bayes Recall:\", gnb_recall)\n","print(\"Gaussian Naive Bayes F1 Score:\", gnb_f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GdM3MjIb_QUy","executionInfo":{"status":"ok","timestamp":1681463647020,"user_tz":-60,"elapsed":5,"user":{"displayName":"Saurabh Bhardwaj","userId":"16519378442147256210"}},"outputId":"3e1c2f01-e972-4e7b-cca7-4f3fb8cce563"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Bernoulli Naive Bayes Accuracy: 0.8839380364047911\n","Bernoulli Naive Bayes Precision: 0.8860911270983214\n","Bernoulli Naive Bayes Recall: 0.815223386651958\n","Bernoulli Naive Bayes F1 Score: 0.8491812697500718\n","---------------------------------------------------------\n","Multinomial Naive Bayes Accuracy: 0.7863496180326323\n","Multinomial Naive Bayes Precision: 0.7440273037542662\n","Multinomial Naive Bayes Recall: 0.7214561500275786\n","Multinomial Naive Bayes F1 Score: 0.7325679081489778\n","---------------------------------------------------------\n","Gaussian Naive Bayes Accuracy: 0.8217730830896915\n","Gaussian Naive Bayes Precision: 0.7012096774193548\n","Gaussian Naive Bayes Recall: 0.9591836734693877\n","Gaussian Naive Bayes F1 Score: 0.8101560680177031\n"]}]},{"cell_type":"markdown","source":["**Discussion and Conclusion**\n","\n","In the above code, we implemented Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. We then used 10-fold cross-validation to evaluate the performance of each classifier on the digits dataset.\n","\n","From the results obtained, we can see that all three variants of Naive Bayes achieved high accuracy, with Bernoulli Naive Bayes achieving the highest accuracy of 0.88 (+/- 0.03), followed by Gaussian Naive Bayes with an accuracy of 0.81 (+/- 0.03), and Multinomial Naive Bayes with an accuracy of 0.78 (+/- 0.03).\n","\n","The reason Bernoulli Naive Bayes performed the best might be because the dataset is a binary classification problem, with each pixel value being either 0 or 1, which makes the binary assumption of Bernoulli Naive Bayes a good fit for the dataset.\n","\n","However, one limitation of Naive Bayes that we observed is that it makes the strong assumption that the features are independent, which may not always hold true in real-world datasets. In addition, Naive Bayes can also suffer from the problem of overfitting if the training data is too small or if the feature space is too large.\n","\n","Here are some suggestions to improve the accuracy of the Naive Bayes classifiers:\n","\n","**Feature engineering:** One way to improve accuracy is to use feature engineering techniques to create new features that capture more information about the dataset. For example, in the case of text classification, you could use techniques such as TF-IDF to weight the importance of different words in the text.\n","\n","**Hyperparameter tuning:** The scikit-learn library provides various hyperparameters that can be tuned to improve the performance of the Naive Bayes classifier. For example, for the Bernoulli Naive Bayes classifier, we can tune the binarization threshold or the alpha value. For Gaussian Naive Bayes, we can adjust the var_smoothing hyperparameter.\n","\n","**Handling missing values:** If the dataset has missing values, we could use techniques such as mean imputation, median imputation, or mode imputation to fill in the missing values before training the classifier.\n","\n","**Other algorithms:** Finally, it's important to note that Naive Bayes is not always the best algorithm for every type of dataset. Therefore, it's worth exploring other algorithms such as decision trees, random forests, or support vector machines to see if they perform better on the given dataset.\n","\n","In conclusion, Naive Bayes classifiers are simple and fast machine learning algorithms that can perform well on certain types of datasets. While they have some limitations, they can be a useful tool in a data scientist's toolbox. For future work, one could explore different types of datasets and compare the performance of Naive Bayes with other machine learning algorithms to get a better understanding of its strengths and weaknesses."],"metadata":{"id":"S60ZKzMBE6XQ"}},{"cell_type":"code","source":[],"metadata":{"id":"SVWoWsE2_mbF"},"execution_count":null,"outputs":[]}]}